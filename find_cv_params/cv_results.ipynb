{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier, AdaBoostRegressor, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data_new/train_pre.csv').drop_duplicates()\n",
    "df_test = pd.read_csv('../data_new/test_pre.csv').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raceId', 'year', 'circuitId', 'weather_warm', 'weather_cold',\n",
       "       'weather_dry', 'weather_wet', 'weather_cloudy', 'driverId',\n",
       "       'constructorId', 'grid', 'results_positionOrder', 'circuit_country',\n",
       "       'constructor_wins', 'constructor_nationality', 'driver_nationality',\n",
       "       'driver_wins', 'driver_age', 'results_points'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(actual, pred, margin):\n",
    "    index = np.argmin(pred)\n",
    "    if actual[index] <= margin:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrays(prediction, actual):\n",
    "    temp_actual, temp_predicton = [], []\n",
    "\n",
    "    for i in prediction:\n",
    "        if i<=3:\n",
    "            temp_predicton.append(1)\n",
    "        else:\n",
    "            temp_predicton.append(0)\n",
    "    \n",
    "    for i in actual:\n",
    "        if(i<=3):\n",
    "            temp_actual.append(1)\n",
    "        else:\n",
    "            temp_actual.append(0)\n",
    "    \n",
    "\n",
    "    return temp_actual, temp_predicton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_podium(prediced_scores):\n",
    "    d = {}\n",
    "    for i in range(len(prediced_scores)):\n",
    "        d[prediced_scores[i]] = i\n",
    "\n",
    "    prediced_scores[::-1].sort()\n",
    "\n",
    "    podium = np.zeros(len(prediced_scores))\n",
    "\n",
    "    for i in range(len(prediced_scores)):\n",
    "        podium[d[prediced_scores[i]]] = i + 1\n",
    "    \n",
    "    return podium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeRegression(X_train, Y_train):\n",
    "    model = Ridge(alpha = 1000, solver = 'svd')\n",
    "    model.fit(X_train, Y_train)        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ridge_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = ridgeRegression(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassoRegression(X_train, Y_train):\n",
    "    model = Lasso(alpha = 0.1, selection = 'cyclic')\n",
    "    model.fit(X_train, Y_train)        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lasso_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = lassoRegression(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTregressor(X_train, Y_train):\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DT_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = DTregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFregressor(X_train, Y_train):\n",
    "    model = RandomForestRegressor(max_depth = 10, min_samples_split = 10, n_estimators = 100)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RF_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = RFregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVregressor(X_train, Y_train):\n",
    "    model = SVR()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SV_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = SVregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPregressor(X_train, Y_train):\n",
    "    model = MLPRegressor(activation = 'logistic', batch_size = 64, hidden_layer_sizes = (256, 32), learning_rate_init = 0.001)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = MLPregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baggingregressor(X_train, Y_train):\n",
    "    model = BaggingRegressor(base_estimator = RandomForestRegressor(max_depth = 10, min_samples_split = 10, n_estimators = 100))\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bagging_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = Baggingregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoostingregressor(X_train, Y_train):\n",
    "    model = AdaBoostRegressor(base_estimator = RandomForestRegressor(max_depth = 10, min_samples_split = 10, n_estimators = 100))\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ada_Boosting_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = AdaBoostingregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        prediction = get_podium(prediction)\n",
    "        actual = X_test['results_positionOrder'].to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling all regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_scores = {}\n",
    "regression_scores['Ridge Regression'] = get_Ridge_Regression_Score()\n",
    "regression_scores['Lasso Regression'] = get_Lasso_Regression_Score()\n",
    "regression_scores['DT Regression'] = get_DT_Regression_Score()\n",
    "regression_scores['RF Regression'] = get_RF_Regression_Score()\n",
    "regression_scores['SV Regression'] = get_SV_Regression_Score()\n",
    "regression_scores['MLP Regression'] = get_MLP_Regression_Score()\n",
    "regression_scores['Bagging Regression'] = get_Bagging_Regression_Score()\n",
    "regression_scores['Adaboost Regression'] = get_Ada_Boosting_Regression_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ridge Regression</th>\n",
       "      <th>Lasso Regression</th>\n",
       "      <th>DT Regression</th>\n",
       "      <th>RF Regression</th>\n",
       "      <th>SV Regression</th>\n",
       "      <th>MLP Regression</th>\n",
       "      <th>Bagging Regression</th>\n",
       "      <th>Adaboost Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.565833</td>\n",
       "      <td>0.147795</td>\n",
       "      <td>0.587452</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.607690</td>\n",
       "      <td>0.598690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.886854</td>\n",
       "      <td>0.887604</td>\n",
       "      <td>0.308191</td>\n",
       "      <td>0.888270</td>\n",
       "      <td>0.884037</td>\n",
       "      <td>0.893967</td>\n",
       "      <td>0.895775</td>\n",
       "      <td>0.892358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.632500</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.566190</td>\n",
       "      <td>0.250445</td>\n",
       "      <td>0.603670</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.614873</td>\n",
       "      <td>0.609963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winner_score</th>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podium_score</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Ridge Regression  Lasso Regression  DT Regression  \\\n",
       "Precision             0.563333          0.565833       0.147795   \n",
       "Accuracy              0.886854          0.887604       0.308191   \n",
       "Recall                0.563333          0.566667       0.860000   \n",
       "F1                    0.563333          0.566190       0.250445   \n",
       "winner_score          0.460000          0.450000       0.590000   \n",
       "podium_score          0.800000          0.800000       0.880000   \n",
       "\n",
       "              RF Regression  SV Regression  MLP Regression  \\\n",
       "Precision          0.587452       0.553333        0.586667   \n",
       "Accuracy           0.888270       0.884037        0.893967   \n",
       "Recall             0.632500       0.553333        0.586667   \n",
       "F1                 0.603670       0.553333        0.586667   \n",
       "winner_score       0.530000       0.490000        0.480000   \n",
       "podium_score       0.890000       0.830000        0.860000   \n",
       "\n",
       "              Bagging Regression  Adaboost Regression  \n",
       "Precision               0.607690             0.598690  \n",
       "Accuracy                0.895775             0.892358  \n",
       "Recall                  0.629167             0.632500  \n",
       "F1                      0.614873             0.609963  \n",
       "winner_score            0.520000             0.550000  \n",
       "podium_score            0.910000             0.910000  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(regression_scores, index = ['Precision', 'Accuracy', 'Recall', 'F1', 'winner_score', 'podium_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(X_train, Y_train):\n",
    "    model = LogisticRegression(max_iter=2500, multi_class = 'ovr')\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Logistic_Regression_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = logisticRegression(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianNB(X_train, Y_train):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_NB_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = gaussianNB(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_classifier(X_train, Y_train):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DT_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = DT_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(X_train, Y_train):\n",
    "    model = RandomForestClassifier(max_depth=20, min_samples_split=10, n_estimators=100)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RF_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = RF_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SV_classifier(X_train, Y_train):\n",
    "    model = SVC()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SV_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = SV_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_classifier(X_train, Y_train):\n",
    "    model = MLPClassifier(activation = 'identity', alpha = 0.001, hidden_layer_sizes = (256, 32), solver = 'adam')\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = MLP_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging_classifier(X_train, Y_train):\n",
    "    model = BaggingClassifier(base_estimator = RandomForestClassifier(max_depth=20, min_samples_split=10, n_estimators=100))\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bagging_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = Bagging_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ada_Boosting_classifier(X_train, Y_train):\n",
    "    model = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=20, min_samples_split=10, n_estimators=100))\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ada_Boosting_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = Ada_Boosting_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score1 += get_score(prediction, actual, 1)\n",
    "        score2 += get_score(prediction, actual, 3)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()), score1/len(df_test['raceId'].unique()), score2/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling all classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nipun\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classification_scores = {}\n",
    "classification_scores['Logistic Regression'] = get_Logistic_Regression_Score()\n",
    "classification_scores['NB classification'] = get_Gaussian_NB_Score()\n",
    "classification_scores['DT classification'] = get_DT_classifier_Score()\n",
    "classification_scores['RF classification'] = get_RF_classifier_Score()\n",
    "classification_scores['SV classification'] = get_SV_classifier_Score()\n",
    "classification_scores['MLP classification'] = get_MLP_classifier_Score()\n",
    "classification_scores['Bagging classification'] = get_Bagging_classifier_Score()\n",
    "classification_scores['Ada Boosting classification'] = get_Ada_Boosting_classifier_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>NB classification</th>\n",
       "      <th>DT classification</th>\n",
       "      <th>RF classification</th>\n",
       "      <th>SV classification</th>\n",
       "      <th>MLP classification</th>\n",
       "      <th>Bagging classification</th>\n",
       "      <th>Ada Boosting classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.518321</td>\n",
       "      <td>0.518619</td>\n",
       "      <td>0.559667</td>\n",
       "      <td>0.559143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540952</td>\n",
       "      <td>0.577262</td>\n",
       "      <td>0.529357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.887179</td>\n",
       "      <td>0.883279</td>\n",
       "      <td>0.870949</td>\n",
       "      <td>0.882999</td>\n",
       "      <td>0.866636</td>\n",
       "      <td>0.894698</td>\n",
       "      <td>0.888123</td>\n",
       "      <td>0.877971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.596667</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536667</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.502431</td>\n",
       "      <td>0.516849</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.621810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499238</td>\n",
       "      <td>0.639452</td>\n",
       "      <td>0.605913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winner_score</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podium_score</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Logistic Regression  NB classification  DT classification  \\\n",
       "precision                0.518321           0.518619           0.559667   \n",
       "accuracy                 0.887179           0.883279           0.870949   \n",
       "recall                   0.566667           0.596667           0.482500   \n",
       "f1                       0.502431           0.516849           0.483516   \n",
       "winner_score             0.550000           0.610000           0.410000   \n",
       "podium_score             0.720000           0.780000           0.690000   \n",
       "\n",
       "              RF classification  SV classification  MLP classification  \\\n",
       "precision              0.559143           0.000000            0.540952   \n",
       "accuracy               0.882999           0.866636            0.894698   \n",
       "recall                 0.741667           0.000000            0.536667   \n",
       "f1                     0.621810           0.000000            0.499238   \n",
       "winner_score           0.860000           0.000000            0.520000   \n",
       "podium_score           0.940000           0.000000            0.720000   \n",
       "\n",
       "              Bagging classification  Ada Boosting classification  \n",
       "precision                   0.577262                     0.529357  \n",
       "accuracy                    0.888123                     0.877971  \n",
       "recall                      0.758333                     0.735000  \n",
       "f1                          0.639452                     0.605913  \n",
       "winner_score                0.880000                     0.770000  \n",
       "podium_score                0.950000                     0.940000  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_scores, index = ['precision', 'accuracy', 'recall', 'f1', 'winner_score', 'podium_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "affa1e01f2d63dc644a5d836267b8cc21782c0351696aa19b5d442d7e4c7d376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
