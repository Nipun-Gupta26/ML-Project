{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../data_new/train_pre.csv').drop_duplicates()\n",
    "df_test = pd.read_csv('../../data_new/test_pre.csv').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_var(stds):\n",
    "    n = 3 # size of each group\n",
    "    return np.sqrt(sum((n-1)*(stds**2))/ len(stds)*(n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_results(gs_model, gs_params):\n",
    "    df = pd.DataFrame(gs_model.cv_results_)\n",
    "    results = ['mean_test_score',\n",
    "            'mean_train_score',\n",
    "            'std_test_score', \n",
    "            'std_train_score']\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(gs_params), \n",
    "                            figsize = (5*len(gs_params), 7),\n",
    "                            sharey='row')\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=25)\n",
    "    lw = 2\n",
    "\n",
    "    for idx, (param_name, param_range) in enumerate(gs_params.items()):\n",
    "        grouped_df = df.groupby(f'param_{param_name}')[results]\\\n",
    "            .agg({'mean_train_score': 'mean',\n",
    "                'mean_test_score': 'mean',\n",
    "                'std_train_score': pooled_var,\n",
    "                'std_test_score': pooled_var})\n",
    "\n",
    "        previous_group = df.groupby(f'param_{param_name}')[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=30)\n",
    "        axes[idx].set_ylim(0.0, 1.1)\n",
    "        axes[idx].plot(param_range, grouped_df['mean_train_score'], label=\"Training score\",\n",
    "                    color=\"darkorange\", lw=lw)\n",
    "        axes[idx].fill_between(param_range, grouped_df['mean_train_score'] - grouped_df['std_train_score'],\n",
    "                        grouped_df['mean_train_score'] + grouped_df['std_train_score'], alpha=0.2,\n",
    "                        color=\"darkorange\", lw=lw)\n",
    "        axes[idx].plot(param_range, grouped_df['mean_test_score'], label=\"Cross-validation score\",\n",
    "                    color=\"navy\", lw=lw)\n",
    "        axes[idx].fill_between(param_range, grouped_df['mean_test_score'] - grouped_df['std_test_score'],\n",
    "                        grouped_df['mean_test_score'] + grouped_df['std_test_score'], alpha=0.2,\n",
    "                        color=\"navy\", lw=lw)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle('Validation curves', fontsize=40)\n",
    "    fig.legend(handles, labels, loc=8, ncol=2, fontsize=20)\n",
    "\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(pred, actual):\n",
    "    if np.argmin(pred) == np.argmin(actual):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, x, y):\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    score = 0\n",
    "    for i in x['raceId'].unique():\n",
    "        X_test = x[x['raceId'] == i]\n",
    "        Y_test = y[x[x['raceId'] == i].index]\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        # print(self.position[x[x['raceId'] == i].index])\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(1):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            # print(actual_podium)\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions) \n",
    "        score += get_score(predicted_podium, actual_podium)\n",
    "    return precision/len(x['raceId'].unique()), accuracy/len(x['raceId'].unique()), recall/len(x['raceId'].unique()), f1/len(x['raceId'].unique()), score/len(x['raceId'].unique())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customCrossValidation():\n",
    "\n",
    "    def split(self, x):\n",
    "        indices = []\n",
    "        for i in range(3):\n",
    "            years = x['year'].unique()\n",
    "            np.random.shuffle(years)\n",
    "            val_years = years[:6]\n",
    "            train_years = years[6:]\n",
    "\n",
    "            indices.append((x[x['year'].isin(train_years)].index, x[x['year'].isin(val_years)].index))\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorRegressor() :\n",
    "\n",
    "    def scoring(self, model, x, y):\n",
    "\n",
    "        precision = 0\n",
    "        accuracy = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "        score = 0\n",
    "        for i in x['raceId'].unique():\n",
    "            X_test = x[x['raceId'] == i]\n",
    "            Y_test = y[x[x['raceId'] == i].index]\n",
    "\n",
    "            predicted_scores = model.predict(X_test)\n",
    "            predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "            # print(self.position[x[x['raceId'] == i].index])\n",
    "            actual_podium = self.position[x[x['raceId'] == i].index].to_numpy()\n",
    "            actual_podium = np.argsort(actual_podium)\n",
    "            # print(predicted_scores)\n",
    "\n",
    "            predictions = np.zeros(len(Y_test))\n",
    "            # predictions[predicted_podium] = 1\n",
    "            actual = np.zeros(len(Y_test))\n",
    "            # actual[actual_podium] = 1\n",
    "\n",
    "            for i in range(1):\n",
    "                predictions[predicted_podium[i]] = 1\n",
    "                # print(actual_podium)\n",
    "                actual[actual_podium[i]] = 1\n",
    "\n",
    "            precision += precision_score(actual, predictions)\n",
    "            accuracy += accuracy_score(actual, predictions)\n",
    "            recall += recall_score(actual, predictions)\n",
    "            f1 += f1_score(actual, predictions) \n",
    "            score += get_score(predicted_podium, actual_podium)\n",
    "        \n",
    "        # self.ridge_metrics = {'precision': precision/len(x['raceId'].unique()), 'accuracy': accuracy/len(x['raceId'].unique()), 'recall': recall/len(x['raceId'].unique()), 'f1': f1/len(x['raceId'].unique())}\n",
    "        return precision/len(x['raceId'].unique())\n",
    "\n",
    "    def find_best_param_svr(self, x, y):\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.position = x['results_positionOrder']\n",
    "\n",
    "        splitter = customCrossValidation().split(x)\n",
    "        self.hyper_params = [{'kernel': ['linear', 'rbf', 'sigmoid', 'poly'], 'degree': [3,4,5], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'C': [0.1, 1, 10, 100, 1000]}]\n",
    "\n",
    "        model_svr = SVR()\n",
    "        self.model_cv = GridSearchCV(estimator=model_svr, param_grid=self.hyper_params, scoring=self.scoring, cv = splitter, return_train_score=True, verbose = 3)\n",
    "        self.model_cv.fit(x.drop(columns = ['results_positionOrder']), y)\n",
    "        self.svr_params = self.model_cv.best_params_\n",
    "\n",
    "    def fit_svr(self, x, y):\n",
    "        model = SVR(**self.svr_params)\n",
    "        model.fit(x.drop(columns = ['results_positionOrder']), y)\n",
    "        self.model = model\n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SupportVectorRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    }
   ],
   "source": [
    "sv.find_best_param_svr(df_train.drop(columns = ['results_points']), df_train['results_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.svr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.fit_svr(df_train.drop(columns = ['results_points']), df_train['results_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(sv.model, df_test.drop(columns = ['results_points']), df_test['results_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cv_results(sv.model_cv, sv.hyper_params[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
