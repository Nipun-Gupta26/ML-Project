{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier, GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data_new/train_pre.csv').drop_duplicates()\n",
    "df_test = pd.read_csv('./data_new/test_pre.csv').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raceId', 'year', 'circuitId', 'weather_warm', 'weather_cold',\n",
       "       'weather_dry', 'weather_wet', 'weather_cloudy', 'driverId',\n",
       "       'constructorId', 'grid', 'results_positionOrder', 'circuit_country',\n",
       "       'constructor_wins', 'constructor_nationality', 'driver_nationality',\n",
       "       'driver_wins', 'driver_age', 'results_points'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    4976\n",
       "4      980\n",
       "3      978\n",
       "11     978\n",
       "8      977\n",
       "5      977\n",
       "7      977\n",
       "6      977\n",
       "9      976\n",
       "2      976\n",
       "12     975\n",
       "10     975\n",
       "1      972\n",
       "13     971\n",
       "14     967\n",
       "15     965\n",
       "16     953\n",
       "17     946\n",
       "18     932\n",
       "19     910\n",
       "Name: results_positionOrder, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['results_positionOrder'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression(X_train, Y_train):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, Y_train)        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Linear_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = linearRegression(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(1):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTregressor(X_train, Y_train):\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DT_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = DTregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(3):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "        # print(confusion_matrix(actual, predictions))\n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFregressor(X_train, Y_train):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RF_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = RFregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(3):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVregressor(X_train, Y_train):\n",
    "    model = SVR()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SV_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = SVregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(3):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPregressor(X_train, Y_train):\n",
    "    model = MLPRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = MLPregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(3):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baggingregressor(X_train, Y_train):\n",
    "    model = BaggingRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bagging_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = Baggingregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(3):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoostingregressor(X_train, Y_train):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gradient_Boosting_Regression_Score():\n",
    "    X_train = df_train.drop(columns = ['results_points'])\n",
    "    Y_train = df_train['results_points']\n",
    "    model = GradientBoostingregressor(X_train.drop(columns = ['results_positionOrder']), Y_train)\n",
    "\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i].drop(columns = ['results_points'])\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_points'].to_numpy()\n",
    "\n",
    "        predicted_scores = model.predict(X_test.drop(columns = ['results_positionOrder']))\n",
    "        predicted_podium = np.argsort(predicted_scores)[::-1]\n",
    "        actual_podium = X_test['results_positionOrder'].to_numpy()\n",
    "        actual_podium = np.argsort(actual_podium)\n",
    "        # print(predicted_scores)\n",
    "\n",
    "        predictions = np.zeros(len(Y_test))\n",
    "        # predictions[predicted_podium] = 1\n",
    "        actual = np.zeros(len(Y_test))\n",
    "        # actual[actual_podium] = 1\n",
    "\n",
    "        for i in range(3):\n",
    "            predictions[predicted_podium[i]] = 1\n",
    "            actual[actual_podium[i]] = 1\n",
    "\n",
    "        precision += precision_score(actual, predictions)\n",
    "        accuracy += accuracy_score(actual, predictions)\n",
    "        recall += recall_score(actual, predictions)\n",
    "        f1 += f1_score(actual, predictions)   \n",
    "\n",
    "    return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling all regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_scores = {}\n",
    "regression_scores['Linear Regression'] = get_Linear_Regression_Score()\n",
    "regression_scores['DT Regression'] = get_DT_Regression_Score()\n",
    "regression_scores['RF Regression'] = get_RF_Regression_Score()\n",
    "regression_scores['SV Regression'] = get_SV_Regression_Score()\n",
    "regression_scores['MLP Regression'] = get_MLP_Regression_Score()\n",
    "regression_scores['Bagging Regression'] = get_Bagging_Regression_Score()\n",
    "regression_scores['Gradient Boosting Regression'] = get_Gradient_Boosting_Regression_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear Regression</th>\n",
       "      <th>DT Regression</th>\n",
       "      <th>RF Regression</th>\n",
       "      <th>SV Regression</th>\n",
       "      <th>MLP Regression</th>\n",
       "      <th>Bagging Regression</th>\n",
       "      <th>Gradient Boosting Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.952542</td>\n",
       "      <td>0.863999</td>\n",
       "      <td>0.892778</td>\n",
       "      <td>0.884472</td>\n",
       "      <td>0.890730</td>\n",
       "      <td>0.891074</td>\n",
       "      <td>0.909106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Linear Regression  DT Regression  RF Regression  SV Regression  \\\n",
       "Precision           0.450000       0.476667       0.583333       0.553333   \n",
       "Accuracy            0.952542       0.863999       0.892778       0.884472   \n",
       "Recall              0.450000       0.476667       0.583333       0.553333   \n",
       "F1                  0.450000       0.476667       0.583333       0.553333   \n",
       "\n",
       "           MLP Regression  Bagging Regression  Gradient Boosting Regression  \n",
       "Precision        0.573333            0.573333                      0.640000  \n",
       "Accuracy         0.890730            0.891074                      0.909106  \n",
       "Recall           0.573333            0.573333                      0.640000  \n",
       "F1               0.573333            0.573333                      0.640000  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(regression_scores, index = ['Precision', 'Accuracy', 'Recall', 'F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(pred, actual, margin):\n",
    "    score = 0\n",
    "    pred_winner = np.argmin(pred)\n",
    "    actual_pos = actual[pred_winner]\n",
    "\n",
    "    # print(np.abs(actual_pos))\n",
    "    if(actual_pos <=3):\n",
    "        score+=1\n",
    "    # score = 0\n",
    "    # if np.argmin(pred) == np.argmin(actual):\n",
    "    #     score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrays(prediction, actual):\n",
    "    temp_actual, temp_predicton = [], []\n",
    "\n",
    "    for i in prediction:\n",
    "        if i<=3:\n",
    "            temp_predicton.append(1)\n",
    "        else:\n",
    "            temp_predicton.append(0)\n",
    "    \n",
    "    for i in actual:\n",
    "        if(i<=3):\n",
    "            temp_actual.append(1)\n",
    "        else:\n",
    "            temp_actual.append(0)\n",
    "    \n",
    "\n",
    "    return temp_actual, temp_predicton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(X_train, Y_train):\n",
    "    model = LogisticRegression(max_iter=2500, multi_class = 'ovr')\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Logistic_Regression_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = logisticRegression(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "    accuracy, precision, recall, f1 = 0,0,0,0\n",
    "\n",
    "    score = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianNB(X_train, Y_train):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_NB_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = gaussianNB(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "    accuracy, precision, recall, f1 = 0,0,0,0\n",
    "\n",
    "    score = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_classifier(X_train, Y_train):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DT_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = DT_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "    accuracy, precision, recall, f1 = 0,0,0,0\n",
    "\n",
    "    score = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(X_train, Y_train):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RF_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = RF_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    # return precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())\n",
    "    return score/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7,\n",
       " 0.8834782029547587,\n",
       " 0.5565357142857146,\n",
       " 0.7316666666666661,\n",
       " 0.6190577200577195)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_RF_classifier_Score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SV_classifier(X_train, Y_train):\n",
    "    model = SVC()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SV_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = SV_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "    score = 0\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_classifier(X_train, Y_train):\n",
    "    model = MLPClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = MLP_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging_classifier(X_train, Y_train):\n",
    "    model = BaggingClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bagging_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = Bagging_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Boosting_classifier(X_train, Y_train):\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gradient_Boosting_classifier_Score():\n",
    "    X_train = df_train\n",
    "    Y_train = df_train['results_positionOrder']\n",
    "    model = Gradient_Boosting_classifier(X_train.drop(columns = ['results_positionOrder', 'results_points']), Y_train)\n",
    "\n",
    "    score = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "    for i in df_test['raceId'].unique():\n",
    "        X_test = df_test[df_test['raceId'] == i]\n",
    "        Y_test = df_test[df_test['raceId'] == i]['results_positionOrder']\n",
    "\n",
    "        prediction = model.predict(X_test.drop(columns = ['results_positionOrder', 'results_points']))\n",
    "        actual = Y_test.to_numpy()\n",
    "        temp_actual, temp_prediction = get_arrays(prediction, actual)\n",
    "        score += get_score(prediction, actual, 0)\n",
    "        \n",
    "        \n",
    "        accuracy += accuracy_score(temp_actual, temp_prediction)\n",
    "        precision += precision_score(temp_actual, temp_prediction)\n",
    "        recall += recall_score(temp_actual, temp_prediction)\n",
    "        f1 += f1_score(temp_actual, temp_prediction)\n",
    "\n",
    "    return score/len(df_test['raceId'].unique()), precision/len(df_test['raceId'].unique()), accuracy/len(df_test['raceId'].unique()), recall/len(df_test['raceId'].unique()), f1/len(df_test['raceId'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling all classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_scores = {}\n",
    "classification_scores['Logistic Regression'] = get_Logistic_Regression_Score()\n",
    "classification_scores['NB classification'] = get_Gaussian_NB_Score()\n",
    "classification_scores['DT classification'] = get_DT_classifier_Score()\n",
    "classification_scores['RF classification'] = get_RF_classifier_Score()\n",
    "classification_scores['SV classification'] = get_SV_classifier_Score()\n",
    "classification_scores['MLP classification'] = get_MLP_classifier_Score()\n",
    "classification_scores['Bagging classification'] = get_Bagging_classifier_Score()\n",
    "classification_scores['Gradient Boosting classification'] = get_Gradient_Boosting_classifier_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>NB classification</th>\n",
       "      <th>DT classification</th>\n",
       "      <th>RF classification</th>\n",
       "      <th>SV classification</th>\n",
       "      <th>MLP classification</th>\n",
       "      <th>Bagging classification</th>\n",
       "      <th>Gradient Boosting classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.533083</td>\n",
       "      <td>0.518619</td>\n",
       "      <td>0.558143</td>\n",
       "      <td>0.880504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.474206</td>\n",
       "      <td>0.574476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.889005</td>\n",
       "      <td>0.883279</td>\n",
       "      <td>0.873455</td>\n",
       "      <td>0.541619</td>\n",
       "      <td>0.866636</td>\n",
       "      <td>0.900243</td>\n",
       "      <td>0.848532</td>\n",
       "      <td>0.889820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.596667</td>\n",
       "      <td>0.502500</td>\n",
       "      <td>0.722500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443333</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.731667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.509605</td>\n",
       "      <td>0.516849</td>\n",
       "      <td>0.500929</td>\n",
       "      <td>0.607087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503429</td>\n",
       "      <td>0.533455</td>\n",
       "      <td>0.630849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Logistic Regression  NB classification  DT classification  \\\n",
       "score                 0.650000           0.650000           0.720000   \n",
       "accuracy              0.533083           0.518619           0.558143   \n",
       "precision             0.889005           0.883279           0.873455   \n",
       "recall                0.563333           0.596667           0.502500   \n",
       "f1_score              0.509605           0.516849           0.500929   \n",
       "\n",
       "           RF classification  SV classification  MLP classification  \\\n",
       "score               0.670000           0.300000            0.710000   \n",
       "accuracy            0.880504           0.000000            0.663000   \n",
       "precision           0.541619           0.866636            0.900243   \n",
       "recall              0.722500           0.000000            0.443333   \n",
       "f1_score            0.607087           0.000000            0.503429   \n",
       "\n",
       "           Bagging classification  Gradient Boosting classification  \n",
       "score                    0.690000                          0.770000  \n",
       "accuracy                 0.474206                          0.574476  \n",
       "precision                0.848532                          0.889820  \n",
       "recall                   0.658333                          0.731667  \n",
       "f1_score                 0.533455                          0.630849  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_scores, index = ['score', 'accuracy', 'precision', 'recall', 'f1_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
